"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
"""
import pandas as pd
import pickle
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import sys
import os

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from loguru import logger


def train_categorization_model():
    """–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏"""

    # –ü—É—Ç–∏
    data_path = Path(__file__).parent.parent.parent.parent / "data" / "training" / "transactions_dataset.csv"
    model_dir = Path(__file__).parent.parent / "models"
    model_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"üìÇ Loading dataset from {data_path}")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        df = pd.read_csv(data_path)
        logger.info(f"‚úÖ Loaded {len(df)} transactions")
        logger.info(f"üìä Categories: {df['category'].nunique()}")
        logger.info(f"üìã Category distribution:\n{df['category'].value_counts()}")
    except FileNotFoundError:
        logger.error(f"‚ùå Dataset not found at {data_path}")
        logger.error("Please create the dataset first using the provided CSV template")
        return False

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = df['description'].str.lower()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    y = df['category']

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    logger.info("üî§ Encoding labels...")
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º stratify –∏–∑-–∑–∞ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (88 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è 19 –∫–∞—Ç–µ–≥–æ—Ä–∏–π)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42
    )
    logger.info(f"üìä Train size: {len(X_train)}, Test size: {len(X_test)}")

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (TF-IDF)
    logger.info("üî§ Vectorizing text with TF-IDF...")
    vectorizer = TfidfVectorizer(
        max_features=500,
        ngram_range=(1, 2),  # unigrams –∏ bigrams
        min_df=1,
        analyzer='word'
    )
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    logger.info(f"‚úÖ Vocabulary size: {len(vectorizer.vocabulary_)}")

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("ü§ñ Training Random Forest model...")
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=20,
        min_samples_split=2,
        random_state=42,
        n_jobs=-1
    )
    model.fit(X_train_vec, y_train)
    logger.info("‚úÖ Model trained successfully")

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    logger.info("üìä Evaluating model...")
    y_pred = model.predict(X_test_vec)
    accuracy = accuracy_score(y_test, y_pred)
    logger.info(f"üéØ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç (—Ç–æ–ª—å–∫–æ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤, –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤ —Ç–µ—Å—Ç–µ)
    try:
        unique_labels = sorted(set(y_test) | set(y_pred))
        target_names_filtered = [label_encoder.classes_[i] for i in unique_labels]
        report = classification_report(
            y_test, y_pred,
            labels=unique_labels,
            target_names=target_names_filtered,
            zero_division=0
        )
        logger.info(f"üìã Classification Report:\n{report}")
    except Exception as e:
        logger.warning(f"Could not generate classification report: {e}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("üíæ Saving model...")
    model_path = model_dir / "categorization_model.pkl"
    vectorizer_path = model_dir / "vectorizer.pkl"
    encoder_path = model_dir / "label_encoder.pkl"

    with open(model_path, "wb") as f:
        pickle.dump(model, f)
    logger.info(f"‚úÖ Model saved to {model_path}")

    with open(vectorizer_path, "wb") as f:
        pickle.dump(vectorizer, f)
    logger.info(f"‚úÖ Vectorizer saved to {vectorizer_path}")

    with open(encoder_path, "wb") as f:
        pickle.dump(label_encoder, f)
    logger.info(f"‚úÖ Label encoder saved to {encoder_path}")

    # Feature importance (—Ç–æ–ø-10 —Ñ–∏—á–µ–π)
    feature_names = vectorizer.get_feature_names_out()
    importances = model.feature_importances_
    top_indices = importances.argsort()[-10:][::-1]

    logger.info("üîù Top 10 most important features:")
    for idx in top_indices:
        logger.info(f"  - {feature_names[idx]}: {importances[idx]:.4f}")

    logger.info("‚úÖ Training completed successfully!")
    logger.info(f"üì¶ Model files saved in: {model_dir}")

    return True


if __name__ == "__main__":
    logger.info("üöÄ Starting ML model training...")
    success = train_categorization_model()
    if success:
        logger.info("‚úÖ Training script completed")
    else:
        logger.error("‚ùå Training failed")
        sys.exit(1)
